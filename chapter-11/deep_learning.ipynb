{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a302ebe",
   "metadata": {},
   "source": [
    "Realizar um modelo de DNN (Deep neural network) n√£o √© t√£o simples. Alguns problemas que podemos ter:\n",
    "- Se deparar com os gradientes de fuga ou explos√£o de gradientes, que ocorre quando os gradientes ficam cada vez menores ou maiores quando circulam de maneira reversa pela DNN durante o treinamento. Isso dificulta o treinamento das camadas inferiores.\n",
    "- Possibilidade de n√£o ter dados de treinamento suficiente para uma rede t√£o grande ou pode custar os olhos da cara para rotular esses dados.\n",
    "- Treinamento pode ser extremamente demorado\n",
    "- Modelos com milh√µes de par√¢metros correm o risco de overfitting no conjunto de treinamento, principalmente se n√£o tiver inst√¢ncias de treinamento suficientes ou elas tiverem muito ru√≠do\n",
    "\n",
    "# Problemas de gradientes de fuga e explos√£o de gradientes\n",
    "\n",
    "Quando o algoritmo de retropropaga√ß√£o passa da camada de sa√≠da para a camada de entrada, propagando o gradiente do erro ao longo do caminho, ele calcula o gradiente da fun√ß√£o de custo em rela√ß√£o a cada par√¢metro na rede e usa esses gradientes para atualizar cada par√¢metro com uma etapa do GD.\n",
    "\n",
    "Como os gradientes costumam ficar cada vez menores, a atualiza√ß√£o do GD deixa os pesos de conex√£o das camadas inferiores praticamente inalterados e o treinamento nunca converge para uma boa solu√ß√£o. Isso se chama _gradiente de fuga_. √Äs vezes pode acontecer o inverso, onde os gradientes podem aumentar cada vez mais at√© as camadas receberem atualiza√ß√µes extremamente grandes de peso e o algoritmo divergir (Explos√£o de divergentes). Em geral, redes neurais profundas sofrem de gradientes inst√°veis.\n",
    "\n",
    "# Inicializa√ß√£o de Glorot e inicializa√ß√£o de He\n",
    "\n",
    "Para remediar os gradientes inst√°veis , Glorot e Bengio (link do artigo no livro) destacam que precisamos que o sinal flua corretamente nas duas dire√ß√µes: √† frente (forward) ao fazer predi√ß√µes e na dire√ß√£o reversa (reverse) ao retropropagar os gradientes. Para que o sinal flua coretamente, os autores argumentam que precsamos que a vari√¢ncia das sa√≠das de cada camada seja igual a vari√¢ncia de suas entradas, e precisamos que os gradientes tenham vari√¢ncia igual antes e depois de fluir atrav√©s de uma camada na dire√ß√£o reversa.\n",
    "\n",
    "A pinc√≠pio n√£o √© poss√≠vel garantir nem um nem outro, a menos que a camada tenha um n√∫mero igual de entradas (sendo chamados de _fan-in_ e _fan-out_ da camada). Os autores propuseram um bom meio-termo, os pesos de conex√£o de cada camada devem ser inicializados aleatoriamente:\n",
    "\n",
    "### Defini√ß√µes\n",
    "\n",
    "$$\n",
    "fan_{avg} = \\frac{fan_{in} + fan_{out}}{2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Distribui√ß√£o uniforme\n",
    "Pesos ùë§ pertencentes a uma distribui√ß√£o uniforme no intervalo de menos raiz quadrada de tr√™s sobre fan m√©dio at√© raiz quadrada de tr√™s sobre fan-avg.\n",
    "$$\n",
    "W \\sim U\\left(-\\sqrt{\\frac{3}{fan_{avg}}}, \\; \\sqrt{\\frac{3}{fan_{avg}}}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Distribui√ß√£o normal\n",
    "Pesos ùë§ pertencentes a uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia igual a 1 sobre fan-avg.\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\; \\frac{1}{fan_{avg}}\\right)\n",
    "$$\n",
    "\n",
    "Rela√ß√£o de fun√ß√µes de ativa√ß√£o e inicializa√ß√£o:\n",
    "- Glorot: Para nenhuma fun√ß√£o de ativa√ß√£o, ou tanh, log√≠stica, softmax\n",
    "- He: ReLU e variantes\n",
    "- LeCun: SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b95673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esse c√≥digo √© apenas de demonstra√ß√£o e n√£o ser√° executado\n",
    "\n",
    "# Por padr√£o, a Keras usa inicializa√ß√£o de Glorot com uma distribui√ß√£o uniforme. Ao criar a camada, podemos mudar para a inicializa√ß√£o He\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "\n",
    "# Para uma inicializa√ß√£o com uma distribui√ß√£o uniforme, mas com base em fan-avg em vez de fan-in, usar o inicializador VarianceScaling\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fav_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
