{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a302ebe",
   "metadata": {},
   "source": [
    "Realizar um modelo de DNN (Deep neural network) n√£o √© t√£o simples. Alguns problemas que podemos ter:\n",
    "- Se deparar com os gradientes de fuga ou explos√£o de gradientes, que ocorre quando os gradientes ficam cada vez menores ou maiores quando circulam de maneira reversa pela DNN durante o treinamento. Isso dificulta o treinamento das camadas inferiores.\n",
    "- Possibilidade de n√£o ter dados de treinamento suficiente para uma rede t√£o grande ou pode custar os olhos da cara para rotular esses dados.\n",
    "- Treinamento pode ser extremamente demorado\n",
    "- Modelos com milh√µes de par√¢metros correm o risco de overfitting no conjunto de treinamento, principalmente se n√£o tiver inst√¢ncias de treinamento suficientes ou elas tiverem muito ru√≠do\n",
    "\n",
    "# Problemas de gradientes de fuga e explos√£o de gradientes\n",
    "\n",
    "Quando o algoritmo de retropropaga√ß√£o passa da camada de sa√≠da para a camada de entrada, propagando o gradiente do erro ao longo do caminho, ele calcula o gradiente da fun√ß√£o de custo em rela√ß√£o a cada par√¢metro na rede e usa esses gradientes para atualizar cada par√¢metro com uma etapa do GD.\n",
    "\n",
    "Como os gradientes costumam ficar cada vez menores, a atualiza√ß√£o do GD deixa os pesos de conex√£o das camadas inferiores praticamente inalterados e o treinamento nunca converge para uma boa solu√ß√£o. Isso se chama _gradiente de fuga_. √Äs vezes pode acontecer o inverso, onde os gradientes podem aumentar cada vez mais at√© as camadas receberem atualiza√ß√µes extremamente grandes de peso e o algoritmo divergir (Explos√£o de divergentes). Em geral, redes neurais profundas sofrem de gradientes inst√°veis.\n",
    "\n",
    "# Inicializa√ß√£o de Glorot e inicializa√ß√£o de He\n",
    "\n",
    "Para remediar os gradientes inst√°veis , Glorot e Bengio (link do artigo no livro) destacam que precisamos que o sinal flua corretamente nas duas dire√ß√µes: √† frente (forward) ao fazer predi√ß√µes e na dire√ß√£o reversa (reverse) ao retropropagar os gradientes. Para que o sinal flua coretamente, os autores argumentam que precsamos que a vari√¢ncia das sa√≠das de cada camada seja igual a vari√¢ncia de suas entradas, e precisamos que os gradientes tenham vari√¢ncia igual antes e depois de fluir atrav√©s de uma camada na dire√ß√£o reversa.\n",
    "\n",
    "A pinc√≠pio n√£o √© poss√≠vel garantir nem um nem outro, a menos que a camada tenha um n√∫mero igual de entradas (sendo chamados de _fan-in_ e _fan-out_ da camada). Os autores propuseram um bom meio-termo, os pesos de conex√£o de cada camada devem ser inicializados aleatoriamente:\n",
    "\n",
    "### Defini√ß√µes\n",
    "\n",
    "$$\n",
    "fan_{avg} = \\frac{fan_{in} + fan_{out}}{2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Distribui√ß√£o uniforme\n",
    "Pesos ùë§ pertencentes a uma distribui√ß√£o uniforme no intervalo de menos raiz quadrada de tr√™s sobre fan m√©dio at√© raiz quadrada de tr√™s sobre fan-avg.\n",
    "$$\n",
    "W \\sim U\\left(-\\sqrt{\\frac{3}{fan_{avg}}}, \\; \\sqrt{\\frac{3}{fan_{avg}}}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Distribui√ß√£o normal\n",
    "Pesos ùë§ pertencentes a uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia igual a 1 sobre fan-avg.\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\; \\frac{1}{fan_{avg}}\\right)\n",
    "$$\n",
    "\n",
    "Rela√ß√£o de fun√ß√µes de ativa√ß√£o e inicializa√ß√£o:\n",
    "- Glorot: Para nenhuma fun√ß√£o de ativa√ß√£o, ou tanh, log√≠stica, softmax\n",
    "- He: ReLU e variantes\n",
    "- LeCun: SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b95673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esse c√≥digo √© apenas de demonstra√ß√£o e n√£o ser√° executado\n",
    "\n",
    "# Por padr√£o, a Keras usa inicializa√ß√£o de Glorot com uma distribui√ß√£o uniforme. Ao criar a camada, podemos mudar para a inicializa√ß√£o He\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "\n",
    "# Para uma inicializa√ß√£o com uma distribui√ß√£o uniforme, mas com base em fan-avg em vez de fan-in, usar o inicializador VarianceScaling\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fav_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60b723",
   "metadata": {},
   "source": [
    "# Fun√ß√µes de ativa√ß√£o de n√£o satura√ß√£o\n",
    "Parte dos problemas com gradientes inst√°veis se d√£o pela m√° escolha da fun√ß√£o de ativa√ß√£o. A fun√ß√£o de ativa√ß√£o ReLU (que normalmente √© utilizada em boa parte das redes neurais) n√£o √© perfeita, e sofre de _dying ReLUs_, ou seja, durante o treinamento, alguns neur√¥nios \"morrem\" na pr√°tica, parando de gerar a sa√≠da de qualquer coisa que seja diferente de 0.\n",
    "\n",
    "Para solucionar esse problema, conv√©m usar uma variante da ReLU, como a _leaky ReLU_, que cont√©m uma inclina√ß√£o para $x < 0$, onde √© definido pelo hiperpar√¢metro $\\alpha$ \n",
    "\n",
    "$\\text{LeakyReLU}(x) = \\max(\\alpha x, x)$\n",
    "\n",
    "Temos alguns outros tipos de LeakyReLU, como o RReLU que randomiza o $\\alpha$ em um determinado intervalo durante o treinamento e √© fixado em um valor m√©dio durante o teste. PReLU (Parametric ReLU), em que $\\alpha$ √© autorizado a ser aprendido durante o treinamento, se tornando um par√¢metro que pode ser modificado durante a retropropaga√ß√£o, o que √© muito √∫til em conjuntos de imagens grandes, mas que pode ocorrer overfitting em no conjunto de treinamento em datasets pequenos.\n",
    "\n",
    "A fun√ß√£o de ativa√ß√£o ELU (Exponential linear unity) ultrapassou em desempenho todas as variantes da ReLU, ela se parece bastante com a ReLU, mas com algumas diferen√ßas:\n",
    "- Ela assume valores negativos (ReLU transforma o gradiente da fun√ß√£o em 0 quando a entrada √© negativa) quando $x < 0$, possibilitando ter uma sa√≠da m√©dia pr√≥xima de 0 e ajuda a remediar o problema dos gradientes em fuga. O hiperpar√¢metro $\\alpha$ define o valor que a fun√ß√£o ELU se aproxima, quando $x$ √© um n√∫mero negativo grande. Geralmente √© definido como 1, mas pode ser alterado como qualquer hiperpar√¢metro.\n",
    "- Tem um gradiente diferente de 0 para $x < 0$, o que evita o problema dos neur√¥nios mortos.\n",
    "- Se $\\alpha$ √© igual a 1, a fun√ß√£o √© suavizada em todos os lugares, incluindo $x = 0$, coisa que ajuda a acelerar o gradiente descendente pois n√£o oscila tanto para a esquerda e direita de $x = 0$\n",
    "\n",
    "***\n",
    "Equa√ß√£o ELU:\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{se } x \\ge 0 \\\\\n",
    "\\alpha (e^x - 1) & \\text{se } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "O principal problema da fun√ß√£o de ativa√ß√£o ELU √© que ela √© mais lenta que a fun√ß√£o ReLU e variantes, devido ao uso da fun√ß√£o exponencial. Sua taxa de converg√™ncia √© mais r√°pida durante o treinamento, mas ainda assim no teste, uma rede ELU ser√° mais lenta que uma rede ReLU.\n",
    "***\n",
    "\n",
    "Scaled ELU (SELU) √© uma variante escalonada da ELU, onde os autores demonstraram que construir uma rede neural composta exclusivamente de uma pilha de camadas densas e, se todas as camadas ocultas usarem a fun√ß√£o de ativa√ß√£o SELU, a rede se normalizar√° automaticamente. A sa√≠da de cada camada tende a preservar uma m√©dia 0 e um desvio-padr√£o 1 durante o treinamento, o que resolve o problema de fuga e explos√£o.\n",
    "Condi√ß√µes para que a autonormaliza√ß√£o ocorra:\n",
    "- Caracter√≠sticas devem estar padronizadas (StandardScaler)\n",
    "- Os pesos de cada camada oculta devem ser inicializados com a inicializa√ß√£o normal de LeCun = kernel_initializer='lecun_normal'\n",
    "- A arquitetura da rede **deve ser sequencial**. SELU em redes n√£o sequenciais , como redes recorrentes ou com _skip connections_, a autonormaliza√ß√£o n√£o ser√° garantida.\n",
    "- O artigo apenas assegura a autonormaliza√ß√£o se todas as camadas forem densas, mas alguns pesquisadores observaram que a fun√ß√£o de ativa√ß√£o SELU tamb√©m pode melhorar o desempenho em redes neurais convolucionais.\n",
    "\n",
    "Ent√£o basicamente: SELU > ELU > leaky ReLU > ReLU > tanh > log√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para utilizar a ativa√ß√£o leaky ReLU, √© s√≥ criar uma camada LeakyReLU() e adicionar ao modelo ap√≥s a camada √† qual quer aplicar\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # [...]\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=.2)\n",
    "    # [...]\n",
    "])\n",
    "\n",
    "# Para a PReLU, substitua o alpha por PReLU().\n",
    "\n",
    "#SELU defina activation='selu' e kernel_initializer='lecun_normal' quando criar a camada\n",
    "layer = keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059b685",
   "metadata": {},
   "source": [
    "# Normaliza√ß√£o em batch\n",
    "Embora o uso da inicializa√ß√£o He junto com a ELU (ou variantes da ReLU) possa reduzir significativamente o risco de problemas, de gradiente de fuga/explos√£o de gradientes no in√≠cio do treinamento, isso n√£o garante que eles n√£o voltem durante o treinamento.\n",
    "\n",
    "A normaliza√ß√£o em batch (BN) aborda esses problams, consistindo em adicionar uma oper√ß√£o no modelo logo antes ou depois da fun√ß√£o de ativa√ß√£o de cada camada oculta. Essa opera√ß√£o centraliza o zero e normaliza cada entrada, em seguida escalona e modifica o resultando usando dois novos vetores de par√¢metros por camada: um para o escalonamento e outro para o deslocamento.\n",
    "\n",
    "### Equa√ß√µes\n",
    "\n",
    "Dado um batch \\( B = \\{x_1, x_2, ..., x_m\\} \\):\n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Explica√ß√£o\n",
    "\n",
    "- $x_i$: valor de entrada de um neur√¥nio para o exemplo $i$  \n",
    "- $m$: tamanho do batch  \n",
    "- $\\mu_B$: m√©dia do batch (centraliza os dados)  \n",
    "- $\\sigma_B^2$: vari√¢ncia do batch (mede dispers√£o)  \n",
    "- $\\epsilon$: constante pequena (evita divis√£o por zero)  \n",
    "- $\\hat{x}_i$: valor normalizado (m√©dia 0, vari√¢ncia 1)  \n",
    "- $\\gamma$: par√¢metro trein√°vel de escala  \n",
    "- $\\beta$: par√¢metro trein√°vel de deslocamento  \n",
    "- $y_i$: sa√≠da final ap√≥s normaliza√ß√£o e ajuste  \n",
    "\n",
    "Durante o treinamento, a BN padroniza suas entradas, em seguida as reescalona e as desloca. Mas na hora de testar, talvez tenhamos que fazer predi√ß√µes para as inst√¢ncias individuais, e n√£o para os batches de inst√¢ncias. Nesse caso n√£o teremos como calcular a m√©dia e desvio-padr√£o de cada entrada. Al√©m do mais, ainda que tenhamos um batch de inst√¢ncias, ele pode ser muito pequeno, ou as inst√¢ncias podem n√£o ser independentes e distribu√≠das de forma id√™ntica, logo, os c√°lculos computacionais em rela√ß√£o ao batch de inst√¢ncia s√£o duvidosos. Uma solu√ß√£o seria aguardar at√© o fim do treinamento, rodar todo o conjunto de treinamento pela rede neural, e calcular a m√©dia e o desvio padr√£o de cada entrada da camada BN. Assim, essas m√©dias de entrada \"finais\" e desvios-padr√£o podem ser utilizados em vez das m√©dias de entreda do batch e desvios-padr√£o ao fazer as predi√ß√µes. Contudo, a maioria das implementa√ß√µes da normaliza√ß√£o em batch estima essas estat√≠sticas finais durante o treinamento, utilizando uma m√©dia m√≥vel de entrada e dos desvios-padr√£o da camada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7314b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
